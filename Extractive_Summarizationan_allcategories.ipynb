{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Create the .kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy kaggle.json to the .kaggle directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set permissions for the Kaggle API token\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Confirm Kaggle API setup\n",
        "!kaggle datasets list -s \"bbc-news-summary\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D4EJotWfei8",
        "outputId": "485a85f8-1376-46c4-aa00-cd10fc92497d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                           title                                                 size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------------  --------------------------------------------------  ------  -------------------  -------------  ---------  ---------------  \n",
            "pariza/bbc-news-summary                                       BBC News Summary                                       9MB  2018-05-06 11:08:19          14966        177  0.75             \n",
            "jacopoferretti/bbc-articles-dataset                           BBC Articles Dataset with Extra Features               4MB  2024-11-11 17:50:09           1235         34  1.0              \n",
            "bhavikjikadara/bbc-news-articles                              BBC News Articles                                      3MB  2024-07-04 08:45:12            564         17  1.0              \n",
            "dignity45/bbc-news-summarycsv-format                          BBC NEWS SUMMARY(CSV FORMAT)                           2MB  2024-09-09 12:57:06             20          2  0.7058824        \n",
            "imumerfarooq/bbc-news-summary                                 BBC News Summary                                       9MB  2023-07-15 06:23:31             14          1  0.125            \n",
            "fadyelkbeer/arabic-summarization-bbc-news                     Arabic summarization (BBC News)                       73MB  2021-12-29 09:03:11            379          7  0.1764706        \n",
            "jammikunal/sanskrit-data                                      Sanskrit Data                                          6MB  2024-06-03 12:14:20              8          0  0.7058824        \n",
            "techsalerator/new-events-data-in-united-kingdom               New Events Data in United Kingdom                      5KB  2024-09-14 09:02:45              2          0  0.64705884       \n",
            "aggle6666/bbc-news                                            BBC News Articles                                    961KB  2022-06-17 08:29:18             56          1  0.5294118        \n",
            "aayushchamoli/bbc-news                                        bbc news                                               9MB  2024-11-08 16:38:46              1          0  0.25             \n",
            "marwanurtaj/busum-bnlp-dataset-multi-document-bangla-summary  BUSUM-BNLP Dataset (Multi-Document Bangla Summary)  1023KB  2023-10-11 10:16:56             56          6  0.75             \n",
            "ankansaha060/bbc-news-dataset-for-extractive-summarization    BBC News Dataset for Extractive Summarization          2MB  2023-09-06 15:44:24             26          0  0.23529412       \n",
            "lakhanmalviya/bbc-news-summary-dataset                        bbc_news_summary_dataset                               3MB  2023-04-06 13:22:26              7          0  0.125            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSTYAUwOe_dG",
        "outputId": "245ff9e5-3bc5-4dbf-8df9-7ea268989d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/pariza/bbc-news-summary\n",
            "License(s): CC0-1.0\n",
            "bbc-news-summary.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d pariza/bbc-news-summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip bbc-news-summary.zip -d bbc-news-summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB6m2xPUfJah",
        "outputId": "a2a0158d-e436-4902-fda6-8b0a201dc260"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bbc-news-summary.zip\n",
            "replace bbc-news-summary/BBC News Summary/News Articles/business/001.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSYkWmnRz_-G",
        "outputId": "4aa8c9c6-f48c-461d-8141-3235edf0628c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhlDoKKd1NXW",
        "outputId": "c7e9f86e-14d7-41de-b6b8-eb4f99dfbc8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extractive summarization"
      ],
      "metadata": {
        "id": "H8r1u3XyhBhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import nltk, re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from joblib import Parallel, delayed\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "pbh16Zqigz_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba07429-29f5-4250-bb68-cc532d04fcc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from tqdm import tqdm\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "Y0_A55232rW3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Directory paths"
      ],
      "metadata": {
        "id": "VlOZRGmqGTF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles_dir = \"/content/bbc-news-summary/BBC News Summary/News Articles/\"\n",
        "summaries_dir = \"/content/bbc-news-summary/BBC News Summary/Summaries/\""
      ],
      "metadata": {
        "id": "NVCCrJr_DcLX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read one text file under the business folder"
      ],
      "metadata": {
        "id": "_TpMfVtvDiPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file in read mode\n",
        "with open(\"/content/bbc-news-summary/BBC News Summary/News Articles/business/001.txt\", \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Print the content\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i39tAn3ADpIC",
        "outputId": "3a59481a-2ba5-4b29-dcbd-ffa5ab02d94d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
            "\n",
            "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
            "\n",
            "Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
            "\n",
            "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
            "\n",
            "TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load articles and summaries\n"
      ],
      "metadata": {
        "id": "nuFNGUtJDuO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load articles and summaries with encoding handling\n",
        "# def load_text_files(directory):\n",
        "#     data = {}\n",
        "#     for root, _, files in os.walk(directory):\n",
        "#         for file in files:\n",
        "#             filepath = os.path.join(root, file)\n",
        "#             try:\n",
        "#                 # Attempt to read with UTF-8 encoding\n",
        "#                 with open(filepath, 'r', encoding='utf-8') as f:\n",
        "#                     data[file] = f.read().strip()\n",
        "#             except UnicodeDecodeError:\n",
        "#                 # Fallback to a different encoding if UTF-8 fails\n",
        "#                 with open(filepath, 'r', encoding='ISO-8859-1') as f:\n",
        "#                     data[file] = f.read().strip()\n",
        "#     return data\n",
        "\n",
        "# # Load articles and summaries\n",
        "# articles = load_text_files(articles_dir)\n",
        "# summaries = load_text_files(summaries_dir)\n",
        "\n",
        "# print(f\"Loaded {len(articles)} articles and {len(summaries)} summaries.\")"
      ],
      "metadata": {
        "id": "VJiquZZo4RjL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load all articles or summaries from multiple subfolders\n",
        "def load_text_files_from_all_categories(base_directory):\n",
        "    data = {}\n",
        "    for category in os.listdir(base_directory):  # Loop through categories\n",
        "        category_path = os.path.join(base_directory, category)\n",
        "        if os.path.isdir(category_path):  # Check if it's a directory\n",
        "            for file in os.listdir(category_path):  # Loop through files\n",
        "                file_path = os.path.join(category_path, file)\n",
        "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    data[f\"{category}/{file}\"] = f.read().strip()\n",
        "    return data\n",
        "\n",
        "# Load all articles and summaries\n",
        "articles = load_text_files_from_all_categories(articles_dir)\n",
        "summaries = load_text_files_from_all_categories(summaries_dir)\n",
        "\n",
        "print(f\"Loaded {len(articles)} articles and {len(summaries)} summaries from all categories.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbpgO-Hk0D-y",
        "outputId": "98742a2e-a706-4cfa-a7fa-63ccc78e55ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2225 articles and 2225 summaries from all categories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text(txt: str = \"\"):\n",
        "  sentences = []\n",
        "  sentences = sent_tokenize(txt)\n",
        "  for sentence in sentences:\n",
        "    # remove everthing in the text that is not alphanumeric i.e. letters or numbers\n",
        "    sentence.replace(\"[^a-zA-Z0-9]\", \" \")\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "eguBeWZghGyH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extractive summarization"
      ],
      "metadata": {
        "id": "rbfXoHev0yyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Distance:\n",
        "\n",
        "    Cosine distance is often used as a dissimilarity measure. It's defined as:\n",
        "\n",
        "Cosine Distance=1−Cosine Similarity\n",
        "Cosine Distance=1−Cosine Similarity\n",
        "\n",
        "    This transformation shifts the range of the cosine similarity to [0, 2]:\n",
        "        0 means the vectors are identical (perfect match),\n",
        "        1 means the vectors are orthogonal (no similarity),\n",
        "        2 means the vectors are opposite (completely dissimilar)."
      ],
      "metadata": {
        "id": "r91kDrdvFx_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sentence1, sentence2, stopwords = []):\n",
        "  \"\"\"\n",
        "  This function computes the cosine similarity between two sentences\n",
        "  by representing them as vectors of word occurrences.\n",
        "  \"\"\"\n",
        "  sentence1 = [word.lower() for word in sentence1]\n",
        "  sentence2 = [word.lower() for word in sentence2]\n",
        "  all_words = list(set(sentence1 + sentence2))\n",
        "  vector1 = [0] * len(all_words)\n",
        "  vector2 = [0] * len(all_words)\n",
        "  # First sentence vector\n",
        "  for word in sentence1:\n",
        "    if not word in stopwords:\n",
        "      vector1[all_words.index(word)] += 1\n",
        "  # Second sentence vector\n",
        "  for word in sentence2:\n",
        "    if not word in stopwords:\n",
        "      vector2[all_words.index(word)] += 1\n",
        "  # Vectors cosine similarity\n",
        "  return 1 - cosine_distance(vector1, vector2)"
      ],
      "metadata": {
        "id": "nGzkxbVkEb8t"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences_similarity_matrix(sentences, stopwords_):\n",
        "  \"\"\"\n",
        "  This function will output a matrix where each element (i,j)(i,j) represents the similarity score\n",
        "  between the i-th and j-th sentences, excluding the specified stopwords.\n",
        "  We can use this matrix to observe sentences which have the highest similarity scores.\n",
        "  \"\"\"\n",
        "  similarity_matrix = np.zeros((len(sentences), len(sentences))) # N on N\n",
        "  for i in range(len(sentences)):\n",
        "      for j in range(len(sentences)):\n",
        "        if i != j:\n",
        "          similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j], stopwords_)\n",
        "  return similarity_matrix"
      ],
      "metadata": {
        "id": "ejG_-nm3F9MN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity_after_stopword_removal(txt):\n",
        "  # nltk.download('stopwords')\n",
        "  # nltk.download('punkt_tab')\n",
        "  final_stopwords = stopwords.words('english')\n",
        "  # Read and tokenize txt\n",
        "  sentences = read_text(txt)\n",
        "  # Get similarity matrix by passing the stopwords\n",
        "  sentence_similarity_matrix = sentences_similarity_matrix(sentences, final_stopwords)\n",
        "  return sentence_similarity_matrix, sentences"
      ],
      "metadata": {
        "id": "bqfY6H7mGihJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(sentence_similarity_matrix, top_n, sentences):\n",
        "  \"\"\"\n",
        "  The provided code snippet implements an extractive text summarization technique\n",
        "  using a graph-based ranking algorithm TextRank:\n",
        "  In this graph, each node represents a sentence,\n",
        "  and edges between nodes are weighted by the similarity scores from the matrix.\n",
        "  \"\"\"\n",
        "  summarized_text = []\n",
        "  # Rank sentences in the given similarity matrix\n",
        "  # convert similarity matrix into a graph structure using numpy\n",
        "  sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "  # rank sentences based on their similarity to other sentences using TextRank\n",
        "  scores = nx.pagerank(sentence_similarity_graph)\n",
        "  # Sort the rank of top sentences\n",
        "  ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse = True)\n",
        "  # Get the top n number of sentences based on rank\n",
        "  for i in range(top_n):\n",
        "    summarized_text.append(ranked_sentences[i][1])\n",
        "  # Output the summarized version\n",
        "  summary = \" \".join(summarized_text)\n",
        "  return summary, len(sentences), ranked_sentences"
      ],
      "metadata": {
        "id": "_sPyxOnb1CjJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate BLEU for summaries (already defined)\n",
        "def calculate_bleu_for_summary_4gram(reference, candidate):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    total_score = 0\n",
        "    for cand_sentence in candidate:\n",
        "        tokenized_cand = cand_sentence.split()\n",
        "        sentence_scores = [\n",
        "            sentence_bleu([ref.split()], tokenized_cand, smoothing_function=smoothing,\n",
        "                          weights=(0.25, 0.25, 0.25, 0.25))\n",
        "            for ref in reference\n",
        "        ]\n",
        "        best_score = max(sentence_scores)\n",
        "        total_score += best_score\n",
        "    average_score = total_score / len(candidate)\n",
        "    return average_score\n",
        "\n",
        "# Function to process a single article\n",
        "def process_single_article(file, article, summaries, stopwords_list, top_n=3):\n",
        "    logger.info(f\"Processing file: {file}\")  # Log the file being processed\n",
        "    # Tokenize reference summary\n",
        "    reference_summary = summaries.get(file, \"\").splitlines()\n",
        "\n",
        "    # Tokenize and calculate similarity matrix\n",
        "    sentences = sent_tokenize(article)\n",
        "    similarity_matrix = sentences_similarity_matrix(sentences, stopwords_list)\n",
        "\n",
        "\n",
        "    final_summary, sent_len, ranked_sentences = summarize(similarity_matrix, top_n, sentences)\n",
        "\n",
        "    # Calculate BLEU\n",
        "    bleu_score = calculate_bleu_for_summary_4gram(reference_summary, final_summary.splitlines())\n",
        "    logger.info(f\"Finished processing file: {file}, BLEU Score: {bleu_score:.4f}\")\n",
        "    return {\n",
        "        \"file\": file,\n",
        "        \"summary\": final_summary,\n",
        "        \"bleu\": bleu_score\n",
        "    }\n",
        "\n",
        "# parallel processing all articles\n",
        "def process_all_articles_parallel(articles, summaries, top_n=3, n_jobs=-1):\n",
        "  stopwords_list = stopwords.words('english')\n",
        "  logger.info(f\"Starting parallel processing for {len(articles)} articles...\")\n",
        "\n",
        "  # Add tqdm progress bar\n",
        "  with tqdm(total=len(articles), desc=\"Processing Articles\") as pbar:\n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(lambda file, article: process_single_article(\n",
        "            file, article, summaries, stopwords_list, top_n\n",
        "            ))(file, article) for file, article in articles.items()\n",
        "        )\n",
        "    pbar.update(1)  # Update progress bar\n",
        "\n",
        "  logger.info(f\"Completed processing {len(results)} articles.\")\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "UjpK-BrV054Z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process all articles\n",
        "def process_all_articles(articles, summaries, top_n=3):\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    final_results = []\n",
        "    total_bleu = 0\n",
        "    st = time.time()\n",
        "    for file, article in articles.items():\n",
        "         # Tokenize reference summary\n",
        "        reference_summary = summaries.get(file, \"\").splitlines()\n",
        "        # Tokenize and calculate similarity matrix\n",
        "        sentences = sent_tokenize(article)\n",
        "        similarity_matrix = sentences_similarity_matrix(sentences, stopwords_list)\n",
        "\n",
        "        # Generate summary\n",
        "        final_summary, sent_len, ranked_sentences = summarize(similarity_matrix, top_n, sentences)\n",
        "\n",
        "        # Calculate BLEU\n",
        "        bleu_score = calculate_bleu_for_summary_4gram(reference_summary, final_summary.splitlines())\n",
        "\n",
        "        # Store results\n",
        "        final_results.append({\n",
        "            \"file\": file,\n",
        "            \"summary\": final_summary,\n",
        "            \"bleu\": bleu_score\n",
        "        })\n",
        "\n",
        "        # Accumulate BLEU scores\n",
        "        total_bleu += bleu_score\n",
        "    print(\"Time taken is :\", time.time() - st)\n",
        "\n",
        "    # Compute average BLEU\n",
        "    average_bleu = total_bleu / len(final_results)\n",
        "\n",
        "    return final_results, average_bleu"
      ],
      "metadata": {
        "id": "s5BL_53e6LVc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process articles for all categories\n",
        "results, avg_bleu = process_all_articles(articles, summaries, top_n=3)\n",
        "\n",
        "# Group results by category\n",
        "category_results = {}\n",
        "for result in results:\n",
        "    category = result['file'].split('/')[0]  # Extract category\n",
        "    if category not in category_results:\n",
        "        category_results[category] = []\n",
        "    category_results[category].append(result)\n",
        "\n",
        "# Calculate BLEU scores for each category\n",
        "category_bleu_scores = {}\n",
        "for category, category_data in category_results.items():\n",
        "    category_bleu = sum(r['bleu'] for r in category_data) / len(category_data)\n",
        "    category_bleu_scores[category] = category_bleu\n",
        "    print(f\"Category: {category}, Articles: {len(category_data)}, Average BLEU Score: {category_bleu:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvEul1Rk6PLW",
        "outputId": "7737790a-5d52-4790-e16a-5f8520065183"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken is : 767.3139472007751\n",
            "Category: entertainment, Articles: 386, Average BLEU Score: 0.1971\n",
            "Category: sport, Articles: 511, Average BLEU Score: 0.2132\n",
            "Category: tech, Articles: 401, Average BLEU Score: 0.1287\n",
            "Category: business, Articles: 510, Average BLEU Score: 0.1990\n",
            "Category: politics, Articles: 417, Average BLEU Score: 0.1568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### n_gram = 2 for Business articles"
      ],
      "metadata": {
        "id": "zL12Grjw_Qm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load articles and summaries with encoding handling\n",
        "def load_text_files(directory):\n",
        "    data = {}\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(root, file)\n",
        "            try:\n",
        "                # Attempt to read with UTF-8 encoding\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    data[file] = f.read().strip()\n",
        "            except UnicodeDecodeError:\n",
        "                # Fallback to a different encoding if UTF-8 fails\n",
        "                with open(filepath, 'r', encoding='ISO-8859-1') as f:\n",
        "                    data[file] = f.read().strip()\n",
        "    return data\n",
        "\n",
        "# Load articles and summaries\n",
        "articles_business = load_text_files(articles_dir)\n",
        "summaries_business = load_text_files(summaries_dir)\n",
        "\n",
        "print(f\"Loaded {len(articles_business)} articles and {len(summaries_business)} summaries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkYz5yUg_O9V",
        "outputId": "cc229417-9592-4530-f767-d863706826e6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 511 articles and 511 summaries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_for_summary_bigram(reference, candidate, weights=(0.5, 0.5, 0, 0)):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    total_score = 0\n",
        "    for cand_sentence in candidate:\n",
        "        tokenized_cand = cand_sentence.split()\n",
        "        sentence_scores = [\n",
        "            sentence_bleu([ref.split()], tokenized_cand, smoothing_function=smoothing, weights=weights)\n",
        "            for ref in reference\n",
        "        ]\n",
        "        best_score = max(sentence_scores)  # Take the best match\n",
        "        total_score += best_score\n",
        "    average_score = total_score / len(candidate)  # Normalize by the number of summary sentences\n",
        "    return average_score"
      ],
      "metadata": {
        "id": "u3YK7KtJAaFQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = (0, 1, 0, 0)\n",
        "bleu_score = calculate_bleu_for_summary_bigram(summaries_business, articles_business, weights)\n",
        "print(bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZio63qmAaCR",
        "outputId": "2bd73634-b188-4c57-eacf-3ba2ddf104a6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1000000000000009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate using Rouge"
      ],
      "metadata": {
        "id": "TYp6Wvx__vUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process all articles with ROUGE calculation\n",
        "def process_all_articles_with_rouge(articles, summaries, top_n=3):\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    final_results = []\n",
        "    st = time.time()\n",
        "    for file, article in articles.items():\n",
        "        # Tokenize reference summary\n",
        "        reference_summary = summaries.get(file, \"\")\n",
        "        # Tokenize and calculate similarity matrix\n",
        "        sentences = sent_tokenize(article)\n",
        "        similarity_matrix = sentences_similarity_matrix(sentences, stopwords_list)\n",
        "\n",
        "        # Generate summary\n",
        "        final_summary, sent_len, ranked_sentences = summarize(similarity_matrix, top_n, sentences)\n",
        "\n",
        "        # Calculate ROUGE\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        rouge_scores = scorer.score(reference_summary, final_summary)\n",
        "\n",
        "        # Store results\n",
        "        final_results.append({\n",
        "            \"file\": file,\n",
        "            \"summary\": final_summary,\n",
        "            \"rouge1\": rouge_scores['rouge1'].fmeasure,\n",
        "            \"rouge2\": rouge_scores['rouge2'].fmeasure,\n",
        "            \"rougeL\": rouge_scores['rougeL'].fmeasure\n",
        "        })\n",
        "\n",
        "    print(\"Time taken is :\", time.time() - st)\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "EijAZcoK_tSw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process articles and calculate ROUGE scores\n",
        "results = process_all_articles_with_rouge(articles, summaries, top_n=3)\n",
        "\n",
        "# Group results by category\n",
        "category_results = {}\n",
        "for result in results:\n",
        "    category = result['file'].split('/')[0]  # Extract category\n",
        "    if category not in category_results:\n",
        "        category_results[category] = []\n",
        "    category_results[category].append(result)\n",
        "\n",
        "# Calculate average ROUGE scores for each category\n",
        "category_rouge_scores = {}\n",
        "for category, category_data in category_results.items():\n",
        "    rouge1_avg = sum(r['rouge1'] for r in category_data) / len(category_data)\n",
        "    rouge2_avg = sum(r['rouge2'] for r in category_data) / len(category_data)\n",
        "    rougeL_avg = sum(r['rougeL'] for r in category_data) / len(category_data)\n",
        "\n",
        "    category_rouge_scores[category] = {\n",
        "        \"rouge1\": rouge1_avg,\n",
        "        \"rouge2\": rouge2_avg,\n",
        "        \"rougeL\": rougeL_avg\n",
        "    }\n",
        "\n",
        "    print(f\"Category: {category}, Articles: {len(category_data)}\")\n",
        "    print(f\"  ROUGE-1: {rouge1_avg:.4f}\")\n",
        "    print(f\"  ROUGE-2: {rouge2_avg:.4f}\")\n",
        "    print(f\"  ROUGE-L: {rougeL_avg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVnYRad__dWr",
        "outputId": "4649df72-f60b-4380-d94d-fe73251b70c3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken is : 797.9454424381256\n",
            "Category: entertainment, Articles: 386\n",
            "  ROUGE-1: 0.5005\n",
            "  ROUGE-2: 0.3829\n",
            "  ROUGE-L: 0.3748\n",
            "Category: sport, Articles: 511\n",
            "  ROUGE-1: 0.4933\n",
            "  ROUGE-2: 0.3858\n",
            "  ROUGE-L: 0.3671\n",
            "Category: tech, Articles: 401\n",
            "  ROUGE-1: 0.4360\n",
            "  ROUGE-2: 0.3218\n",
            "  ROUGE-L: 0.3172\n",
            "Category: business, Articles: 510\n",
            "  ROUGE-1: 0.5054\n",
            "  ROUGE-2: 0.3889\n",
            "  ROUGE-L: 0.3721\n",
            "Category: politics, Articles: 417\n",
            "  ROUGE-1: 0.4697\n",
            "  ROUGE-2: 0.3590\n",
            "  ROUGE-L: 0.3411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "szJMu5dDBD6-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}