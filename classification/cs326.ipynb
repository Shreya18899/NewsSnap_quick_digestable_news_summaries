{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to the dataset\n",
    "data_path = \"bbc\"\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load data from each folder\n",
    "for label in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, label)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                data.append(text)\n",
    "                labels.append(label)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'text': data, 'label': labels})\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['entertainment', 'business', 'sport', 'politics', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert labels to numeric format\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = torch.tensor(label_encoder.fit_transform(train_labels))\n",
    "test_labels_encoded = torch.tensor(label_encoder.transform(test_labels))\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class BBCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, self.labels[idx]\n",
    "\n",
    "train_dataset = BBCDataset(train_encodings, train_labels_encoded)\n",
    "test_dataset = BBCDataset(test_encodings, test_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/var/folders/zr/7813fwcx6bg_p3f62s0mpssc0000gn/T/ipykernel_4337/638012667.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, self.labels[idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3323753818902852\n",
      "Epoch 2 Loss: 0.07535161263409204\n",
      "Epoch 3 Loss: 0.06179204572358036\n",
      "Epoch 4 Loss: 0.052049135712357905\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Define DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Define training function\n",
    "def train(model, train_loader, optimizer, epochs=4):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = CrossEntropyLoss()(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "train(model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/7813fwcx6bg_p3f62s0mpssc0000gn/T/ipykernel_4337/638012667.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, self.labels[idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.30%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9730337078651685"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())  # Collect predictions\n",
    "            all_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert-classification-model/tokenizer_config.json',\n",
       " './bert-classification-model/special_tokens_map.json',\n",
       " './bert-classification-model/vocab.txt',\n",
       " './bert-classification-model/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bert-classification-model\")\n",
    "tokenizer.save_pretrained(\"./bert-classification-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Path to the saved model\n",
    "saved_model_path = \"./bert-classification-model\"\n",
    "\n",
    "# Load the saved model\n",
    "model = BertForSequenceClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/7813fwcx6bg_p3f62s0mpssc0000gn/T/ipykernel_4337/638012667.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, self.labels[idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.30%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9730337078651685"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "\n",
    "            # Move inputs and labels to the correct device\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get outputs and predictions\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Call the evaluate function\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/7813fwcx6bg_p3f62s0mpssc0000gn/T/ipykernel_4337/638012667.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, self.labels[idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: china now top trader with japan china overtook the us to become japan's biggest trading partner in 2004, according to numbers released by japan's finance ministry on wednesday. china accounted for 20. 1 % of japan's trade in 2004, compared with 18. 6 % for the us. in 2003, the us was ahead with 20. 5 % and china came second with 19. 2 %. the change highlights china's growing importance as an economic powerhouse. in 2004, japan's imports from and exports to china ( and hong kong ) added up to 22, 201bn yen ( $ 214. 6bn ; £114. 5bn ). this is the highest figure for japanese trade with china since records began in 1947. it compares with 20, 479. 5bn yen in trade with the us. trade with the us during 2004 was hurt by one - off factors, including a 13 - month ban on us beef imports following the discovery of a cow infected with mad cow disease ( bse ) in the us. however, economists predict china will become an even more important japanese trading partner in the coming years. on tuesday, figures showed china's economy grew by 9. 5 % in 2004 and experts say the overall growth picture remains strong. analysts see two spurs to future growth as being china's membership of the world trade organisation and lower trade tariffs. during 2004, japan's trade surplus grew 17. 9 % to 12. 011 trillion yen, with more than half the surplus, 6. 962 trillion yen, accounted for by its trade with the us. in december, the surplus grew 1. 8 % on a year ago to 1. 14 trillion yen thanks to stronger - than - expected exports.\n",
      "True Label: 0\n",
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "def print_example_with_prediction(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Move inputs and labels to the correct device\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get outputs and predictions\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Convert tensor to readable format\n",
    "            example_index = 0  # Change this to see other examples\n",
    "            input_ids = inputs['input_ids'][example_index]\n",
    "            attention_mask = inputs['attention_mask'][example_index]\n",
    "            true_label = labels[example_index].item()\n",
    "            predicted_label = predictions[example_index].item()\n",
    "\n",
    "            # Decode input_ids to get the original text\n",
    "            decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Print details\n",
    "            print(f\"Input Text: {decoded_text}\")\n",
    "            print(f\"True Label: {true_label}\")\n",
    "            print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "            # Stop after one example\n",
    "            break\n",
    "\n",
    "# Call the function\n",
    "print_example_with_prediction(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
