{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi4R0NOv5YXY"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets torch evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rouge_score"
      ],
      "metadata": {
        "id": "SzR55cEHIc9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "FwSeubKTH_JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the .kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy kaggle.json to the .kaggle directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set permissions for the Kaggle API token\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Confirm Kaggle API setup\n",
        "!kaggle datasets list -s \"bbc-news-summary\""
      ],
      "metadata": {
        "id": "ETbw1Nx37FZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d pariza/bbc-news-summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nb28ufp7HNk",
        "outputId": "42e50624-9699-4be8-e6bb-7dbbbfd5a3a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/pariza/bbc-news-summary\n",
            "License(s): CC0-1.0\n",
            "Downloading bbc-news-summary.zip to /content\n",
            " 79% 7.00M/8.91M [00:01<00:00, 8.47MB/s]\n",
            "100% 8.91M/8.91M [00:01<00:00, 6.21MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip bbc-news-summary.zip -d bbc-news-summary"
      ],
      "metadata": {
        "id": "OKg5ZHaG7I4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load articles and summaries\n",
        "def load_bbc_dataset(articles_dir, summaries_dir):\n",
        "    articles = []\n",
        "    summaries = []\n",
        "\n",
        "    # Iterate through categories\n",
        "    for category in os.listdir(articles_dir):\n",
        "        article_path = os.path.join(articles_dir, category)\n",
        "        summary_path = os.path.join(summaries_dir, category)\n",
        "\n",
        "        if os.path.isdir(article_path):\n",
        "            for file in os.listdir(article_path):\n",
        "                article_file = os.path.join(article_path, file)\n",
        "                summary_file = os.path.join(summary_path, file)\n",
        "\n",
        "                if os.path.exists(article_file) and os.path.exists(summary_file):\n",
        "                    with open(article_file, 'r', encoding='utf-8', errors='ignore') as af, \\\n",
        "                         open(summary_file, 'r', encoding='utf-8', errors='ignore') as sf:\n",
        "                        articles.append(af.read().strip())\n",
        "                        summaries.append(sf.read().strip())\n",
        "\n",
        "    return articles, summaries\n",
        "\n",
        "# Paths to articles and summaries\n",
        "articles_dir = \"/content/bbc-news-summary/BBC News Summary/News Articles/\"\n",
        "summaries_dir = \"/content/bbc-news-summary/BBC News Summary/Summaries/\"\n",
        "\n",
        "# Load data\n",
        "articles, summaries = load_bbc_dataset(articles_dir, summaries_dir)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_articles, val_test_articles, train_summaries, val_test_summaries = train_test_split(\n",
        "    articles, summaries, test_size=0.2, random_state=42\n",
        ")\n",
        "val_articles, test_articles, val_summaries, test_summaries = train_test_split(\n",
        "    val_test_articles, val_test_summaries, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Load T5 tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(articles, summaries, tokenizer, max_input_length=512, max_target_length=128):\n",
        "    model_inputs = tokenizer(articles, max_length=max_input_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    labels = tokenizer(summaries, max_length=max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Replace padding token ids in labels with -100 to ignore during training\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# Prepare datasets\n",
        "train_data = preprocess_data(train_articles, train_summaries, tokenizer)\n",
        "val_data = preprocess_data(val_articles, val_summaries, tokenizer)\n",
        "test_data = preprocess_data(test_articles, test_summaries, tokenizer)\n",
        "\n",
        "# Move tensors to GPU\n",
        "train_data = {key: val.to(device) for key, val in train_data.items()}\n",
        "val_data = {key: val.to(device) for key, val in val_data.items()}\n",
        "\n",
        "# Load T5 model and move to GPU\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "\n",
        "# Define training arguments for summarization\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    predict_with_generate=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    eval_steps=500,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        ")\n",
        "\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.input_ids = data[\"input_ids\"]\n",
        "        self.attention_mask = data[\"attention_mask\"]\n",
        "        self.labels = data[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SummarizationDataset(train_data)\n",
        "val_dataset = SummarizationDataset(val_data)\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Load trained model for evaluation\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Generate summaries for test articles\n",
        "generated_summaries = [\n",
        "    summarizer(article, max_length=128, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
        "    for article in test_articles\n",
        "]\n",
        "\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "results = rouge.compute(predictions=generated_summaries, references=test_summaries, use_stemmer=True)\n",
        "\n",
        "# Print ROUGE results\n",
        "for key in results:\n",
        "    print(f\"{key}: {results[key].mid}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "EhzMfDYA-mPo",
        "outputId": "ab8a4a5c-d96e-46ec-eda4-ec4d825a775d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-1-ff1c6d0f663e>:117: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinghshreya196\u001b[0m (\u001b[33msinghshreya196-student\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241202_063730-7s8fl3ts</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/singhshreya196-student/huggingface/runs/7s8fl3ts' target=\"_blank\">results</a></strong> to <a href='https://wandb.ai/singhshreya196-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/singhshreya196-student/huggingface' target=\"_blank\">https://wandb.ai/singhshreya196-student/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/singhshreya196-student/huggingface/runs/7s8fl3ts' target=\"_blank\">https://wandb.ai/singhshreya196-student/huggingface/runs/7s8fl3ts</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='669' max='669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [669/669 04:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.570953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.534389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.831200</td>\n",
              "      <td>0.527357</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.10/dist-packages/datasets/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ff1c6d0f663e>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# Evaluate with ROUGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.10/dist-packages/datasets/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(generated_summaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaeZGH26IHHV",
        "outputId": "a247e5e6-193a-4e2c-aa7f-77933a151ab3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "223"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Compute ROUGE scores\n",
        "results = rouge.compute(predictions=generated_summaries, references=test_summaries, use_stemmer=True)\n",
        "\n",
        "# Print ROUGE results\n",
        "for key in results:\n",
        "    print(f\"{key}: {results[key]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbLNAKAAIDx8",
        "outputId": "26880687-5ade-4d20-ae83-aec1f726c89f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: 0.3755542558833496\n",
            "rouge2: 0.29366889632031407\n",
            "rougeL: 0.2944316313768526\n",
            "rougeLsum: 0.29539844632681955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViLPppDRIhCZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}